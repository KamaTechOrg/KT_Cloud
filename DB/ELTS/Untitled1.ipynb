{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a559b7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Updated: 2024-09-15 01:26:43.642000\n",
      "Column<'UpdatedAt'>\n",
      "Column<'UpdatedAt'>\n",
      "No new data to process.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum, count, coalesce, lit, current_timestamp\n",
    "import sqlite3\n",
    "\n",
    "def incremental_etl():\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Customer ETL\") \\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/path/to/sqlite-jdbc-driver.jar\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    conn = sqlite3.connect('../../../Customers_ETL.db')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        # EXTRACT (Load CSVs using PySpark)\n",
    "        customers_df = spark.read.csv(\"../../../Customer.csv\", header=True, inferSchema=True)\n",
    "        invoices_df = spark.read.csv(\"../../../Invoice.csv\", header=True, inferSchema=True)\n",
    "        invoice_lines_df = spark.read.csv(\"../../../InvoiceLine.csv\", header=True, inferSchema=True)\n",
    "\n",
    "        # Check if the 'customer_loyalty' table exists in SQLite\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT name FROM sqlite_master WHERE type='table' AND name='customer_loyalty_and_invoice_size_ETL';\n",
    "        \"\"\")\n",
    "        table_exists = cursor.fetchone()\n",
    "\n",
    "        # If the table exists, get the latest processed updated_at\n",
    "        if table_exists:\n",
    "            latest_updated_at_query = \"SELECT MAX(updated_at) FROM customer_loyalty_and_invoice_size_ETL\"\n",
    "            cursor.execute(latest_updated_at_query)\n",
    "            last_updated = cursor.fetchone()[0]\n",
    "        else:\n",
    "            last_updated = None\n",
    "\n",
    "        if not last_updated:\n",
    "            last_updated = '1900-01-01 00:00:00'  # Default to a very old timestamp\n",
    "\n",
    "        # Convert last_updated to Spark timestamp\n",
    "        last_updated = spark.sql(f\"SELECT TO_TIMESTAMP('{last_updated}')\").collect()[0][0]\n",
    "        print(f\"Last Updated: {last_updated}\")\n",
    "\n",
    "        # Filter the data for rows that have been created or updated after the last load\n",
    "        customers_filtered_df = customers_df.filter(customers_df['UpdatedAt'] > last_updated)\n",
    "        invoices_filtered_df = invoices_df.filter(invoices_df['UpdatedAt'] > last_updated)\n",
    "        invoice_lines_filtered_df = invoice_lines_df.filter(invoice_lines_df['UpdatedAt'] > last_updated)\n",
    "\n",
    "        # Skip if no new data\n",
    "        if customers_filtered_df.count() == 0 and invoices_filtered_df.count() == 0 and invoice_lines_filtered_df.count() == 0:\n",
    "            print(\"No new data to process.\")\n",
    "            return\n",
    "\n",
    "        # TRANSFORM\n",
    "        # Calculate total spend per invoice\n",
    "        invoice_lines_filtered_df = invoice_lines_filtered_df.withColumn(\n",
    "            'total_spend', col('UnitPrice') * col('Quantity')\n",
    "        )\n",
    "\n",
    "        # Join customer, invoice, and invoice line data\n",
    "        customer_invoices_df = customers_filtered_df.join(invoices_filtered_df, 'CustomerId', how='left')\n",
    "        customer_invoice_lines_df = customer_invoices_df.join(\n",
    "            invoice_lines_filtered_df, 'InvoiceId', how='left'\n",
    "        )\n",
    "\n",
    "        # If table exists, read historical data from SQLite using Spark's JDBC\n",
    "        if table_exists:\n",
    "            historical_data_df = spark.read \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", \"jdbc:sqlite:/path/to/Customers_ETL.db\") \\\n",
    "                .option(\"dbtable\", \"customer_loyalty_and_invoice_size_ETL\") \\\n",
    "                .load()\n",
    "        else:\n",
    "            # Create an empty historical DataFrame if table doesn't exist\n",
    "            historical_data_df = spark.createDataFrame([], schema=customer_invoice_lines_df.schema)\n",
    "\n",
    "        # Aggregation: Calculating loyalty_score and avg_invoice_size\n",
    "        customer_summary_df = customer_invoice_lines_df.groupBy(\n",
    "            'CustomerId', 'FirstName', 'LastName'\n",
    "        ).agg(\n",
    "            count('InvoiceId').alias('loyalty_score'),  # Number of invoices\n",
    "            spark_sum('total_spend').alias('total_spend')  # Total spend\n",
    "        )\n",
    "\n",
    "        # Merge with historical data\n",
    "        transformed_df = customer_summary_df.join(\n",
    "            historical_data_df, on='CustomerId', how='left'\n",
    "        )\n",
    "\n",
    "        # Update loyalty_score and avg_invoice_size\n",
    "        transformed_df = transformed_df.withColumn(\n",
    "            'loyalty_score',\n",
    "            coalesce(col('loyalty_score_x'), lit(0)) + col('loyalty_score')\n",
    "        ).withColumn(\n",
    "            'avg_invoice_size',\n",
    "            col('total_spend') / col('loyalty_score')\n",
    "        ).withColumn(\n",
    "            'created_at',\n",
    "            coalesce(col('created_at'), current_timestamp())\n",
    "        ).withColumn(\n",
    "            'updated_at', current_timestamp()\n",
    "        ).withColumn(\n",
    "            'updated_by', lit('process:ETL')\n",
    "        )\n",
    "\n",
    "        # Final columns for loading into the database\n",
    "        final_df = transformed_df.select(\n",
    "            'CustomerId', 'FirstName', 'LastName', 'loyalty_score', \n",
    "            'avg_invoice_size', 'created_at', 'updated_at', 'updated_by'\n",
    "        )\n",
    "\n",
    "        # LOAD\n",
    "        # Remove existing rows for customers being updated in the SQLite DB\n",
    "        cursor.execute(\"\"\"\n",
    "            DELETE FROM customer_loyalty_and_invoice_size_ETL\n",
    "            WHERE CustomerId IN ({})\n",
    "        \"\"\".format(\",\".join([\"?\"] * final_df.count())), final_df.select('CustomerId').rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "        # Load transformed data into the SQLite database using JDBC\n",
    "        final_df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:sqlite:/path/to/Customers_ETL.db\") \\\n",
    "            .option(\"dbtable\", \"customer_loyalty_and_invoice_size_ETL\") \\\n",
    "            .mode('append') \\\n",
    "            .save()\n",
    "\n",
    "        # Commit the changes to the SQLite database\n",
    "        conn.commit()\n",
    "\n",
    "    finally:\n",
    "        # Close the SQLite connection\n",
    "        conn.close()\n",
    "        # Stop the Spark session\n",
    "        spark.stop()\n",
    "\n",
    "incremental_etl()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f5458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d5a342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import sqlite3  # Assuming you're using sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def load():\n",
    "    # Step 1: Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"CustomerLoyaltyAndInvoiceSize\").getOrCreate()\n",
    "\n",
    "    try:\n",
    "        customers = spark.read.csv(\"../../../Customer.csv\", header=True, inferSchema=True)\n",
    "        invoices = spark.read.csv(\"../../../Invoice.csv\", header=True, inferSchema=True)\n",
    "        invoice_lines = spark.read.csv(\"../../../InvoiceLine.csv\", header=True, inferSchema=True)\n",
    "\n",
    "        # Calculate Loyalty Score: Number of purchases (invoices per customer)\n",
    "        loyalty_score = invoices.groupBy(\"CustomerId\").agg(F.count(\"*\").alias(\"loyalty_score\"))\n",
    "\n",
    "        # Calculate total amount spent per invoice\n",
    "        invoice_totals = invoice_lines.groupBy(\"InvoiceId\").agg(F.sum(F.col(\"UnitPrice\") * F.col(\"Quantity\")).alias(\"total_amount\"))\n",
    "\n",
    "        # Calculate average invoice size per customer by joining with Invoices\n",
    "        avg_invoice_size = invoices.join(invoice_totals, \"InvoiceId\") \\\n",
    "            .groupBy(\"CustomerId\") \\\n",
    "            .agg(F.avg(\"total_amount\").alias(\"avg_invoice_size\"))\n",
    "        \n",
    "        # Join loyalty score and average invoice size with customers\n",
    "        result = customers.join(loyalty_score, \"CustomerId\").join(avg_invoice_size, \"CustomerId\")\n",
    "\n",
    "         # Add metadata columns\n",
    "        result = result.withColumn(\"created_at\", F.current_timestamp()) \\\n",
    "                   .withColumn(\"updated_at\", F.current_timestamp()) \\\n",
    "                   .withColumn(\"updated_by\", F.lit(\"DailyETL:SL\"))\n",
    "        \n",
    "        # Select only the desired columns\n",
    "        final_result = result.select(\"CustomerId\", \"FirstName\", \"LastName\", \"loyalty_score\", \"avg_invoice_size\", \"created_at\", \"updated_at\", \"updated_by\")\n",
    "        \n",
    "        final_df = final_result.toPandas()\n",
    "\n",
    "        conn = sqlite3.connect('../../../Customers_ETL.db')\n",
    "        final_df.to_sql('customer_loyalty_and_invoice_size_ETL', conn, if_exists='replace', index=False)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        spark.stop()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        # Close the SQLite connection and stop Spark session\n",
    "        conn.close()\n",
    "        spark.stop()\n",
    "\n",
    "\n",
    "\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3212506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 'Martha', 'Silk', 8, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(53, 'Phil', 'Hughes', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(34, 'João', 'Fernandes', 7, 5.659999999999999, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(28, 'Julia', 'Barnett', 7, 6.231428571428572, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(27, 'Patrick', 'Gray', 7, 5.374285714285714, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(26, 'Richard', 'Cunningham', 7, 6.802857142857142, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(44, 'Terhi', 'Hämäläinen', 7, 5.945714285714287, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(12, 'Roberto', 'Almeida', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(22, 'Heather', 'Leacock', 7, 5.660000000000001, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(47, 'Lucas', 'Mancini', 7, 5.374285714285714, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(1, 'Luís', 'Gonçalves', 10, 5.660000000000001, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(52, 'Emma', 'Jones', 7, 5.374285714285714, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(13, 'Fernanda', 'Ramos', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(16, 'Frank', 'Harris', 7, 5.374285714285714, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(6, 'Helena', 'Holý', 7, 7.088571428571428, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(3, 'François', 'Tremblay', 7, 5.659999999999999, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(40, 'Dominique', 'Lefebvre', 7, 5.517142857142858, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(20, 'Dan', 'Miller', 7, 5.660000000000001, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(57, 'Luis', 'Rojas', 7, 6.659999999999999, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(54, 'Steve', 'Murray', 7, 5.374285714285714, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(48, 'Johannes', 'Van der Berg', 7, 5.802857142857143, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(5, 'František', 'Wichterlová', 7, 5.802857142857143, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(19, 'Tim', 'Goyer', 7, 5.517142857142858, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(41, 'Marc', 'Dubois', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(15, 'Jennifer', 'Peterson', 7, 5.517142857142857, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(43, 'Isabelle', 'Mercier', 7, 5.802857142857143, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(37, 'Fynn', 'Zimmermann', 7, 6.231428571428572, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(17, 'Jack', 'Smith', 7, 5.659999999999999, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(9, 'Kara', 'Nielsen', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(35, 'Madalena', 'Sampaio', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(4, 'Bjørn', 'Hansen', 7, 5.660000000000001, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(55, 'Mark', 'Taylor', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(59, 'Puja', 'Srivastava', 6, 6.1066666666666665, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(8, 'Daan', 'Peeters', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(23, 'John', 'Gordon', 7, 5.374285714285714, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(39, 'Camille', 'Bernard', 7, 5.517142857142857, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(49, 'Stanisław', 'Wójcik', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(7, 'Astrid', 'Gruber', 7, 6.088571428571427, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(51, 'Joakim', 'Johansson', 7, 5.517142857142856, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(10, 'Eduardo', 'Martins', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(50, 'Enrique', 'Muñoz', 7, 5.374285714285714, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(45, 'Ladislav', 'Kovács', 7, 6.517142857142857, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(38, 'Niklas', 'Schröder', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(25, 'Victor', 'Stevens', 7, 6.088571428571429, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(24, 'Frank', 'Ralston', 7, 6.231428571428572, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(29, 'Robert', 'Brown', 7, 5.374285714285714, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(21, 'Kathy', 'Chase', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(32, 'Aaron', 'Mitchell', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(56, 'Diego', 'Gutiérrez', 7, 5.374285714285714, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(58, 'Manoj', 'Pareek', 7, 5.517142857142857, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(33, 'Ellie', 'Sullivan', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(11, 'Alexandre', 'Rocha', 7, 5.374285714285714, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(14, 'Mark', 'Philips', 7, 5.374285714285714, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(42, 'Wyatt', 'Girard', 7, 5.659999999999999, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(2, 'Leonie', 'Köhler', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(30, 'Edward', 'Francis', 7, 5.374285714285714, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(46, 'Hugh', \"O'Reilly\", 7, 6.517142857142857, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(18, 'Michelle', 'Brooks', 7, 5.3742857142857146, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n",
      "(36, 'Hannah', 'Schneider', 7, 5.374285714285714, '2024-09-15 01:26:43.642000', '2024-09-15 01:26:43.642000', 'DailyETL:SL')\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('../../../Customers_ETL.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query to check if a row with the given CustomerId exists\n",
    "check_query = \"SELECT * FROM customer_loyalty_and_invoice_size_ETL\"\n",
    "\n",
    "# Execute the query with the specific CustomerId\n",
    "cursor.execute(check_query)\n",
    "\n",
    "# Fetch all rows from the result set\n",
    "results = cursor.fetchall()\n",
    "\n",
    "# Print each row\n",
    "for row in results:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90499f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
